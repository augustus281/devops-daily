{
  "id": "monitoring-observability-quiz",
  "title": "Monitoring & Observability Quiz",
  "description": "Master application monitoring, metrics collection, logging, and observability best practices",
  "category": "Monitoring",
  "icon": "Code",
  "totalPoints": 108,
  "theme": {
    "primaryColor": "amber",
    "gradientFrom": "from-amber-500",
    "gradientTo": "to-orange-600"
  },
  "metadata": {
    "estimatedTime": "18-23 minutes",
    "difficultyLevels": {
      "beginner": 3,
      "intermediate": 4,
      "advanced": 2
    },
    "createdDate": "2024-08-15"
  },
  "questions": [
    {
      "id": "metrics-collection",
      "title": "Collecting Application Metrics",
      "description": "Your application needs to track response times, error rates, and throughput. What's the best approach?",
      "situation": "You're running a Node.js web application and need visibility into performance metrics.",
      "codeExample": "# Metrics needed:\n# - Request rate (requests/second)\n# - Response time (p50, p95, p99)\n# - Error rate (%)\n# - Active connections",
      "options": [
        "Use console.log() to print metrics and grep the logs",
        "Manually track metrics in a spreadsheet",
        "Only monitor when problems occur",
        "Instrument code with Prometheus client library, expose /metrics endpoint, and scrape with Prometheus server"
      ],
      "correctAnswer": 3,
      "explanation": "Prometheus is the industry standard for metrics collection. Instrument your code with client libraries to expose metrics at /metrics, then Prometheus scrapes and stores time-series data. This enables powerful querying with PromQL, alerting, and visualization with Grafana. It's designed for high-cardinality metrics and dynamic environments.",
      "hint": "What's the industry-standard time-series database designed specifically for metrics?",
      "difficulty": "beginner",
      "points": 10
    },
    {
      "id": "log-aggregation",
      "title": "Centralizing Logs",
      "description": "Your microservices architecture has 50 containers across 10 servers. How do you search logs?",
      "situation": "Developers need to search logs across all services to debug issues. Currently, they SSH into each server.",
      "options": [
        "SSH into each container and use grep",
        "Email logs to developers daily",
        "Use centralized logging with ELK Stack (Elasticsearch, Logstash, Kibana) or Loki for aggregation, indexing, and search",
        "Store logs only on local disk"
      ],
      "correctAnswer": 2,
      "explanation": "Centralized logging solutions like ELK Stack or Loki aggregate logs from all services into a searchable system. Logs are shipped (via Filebeat, Fluentd, or Promtail), indexed, and made searchable through a web UI. This enables correlation across services, retention policies, and eliminates the need to SSH into individual servers.",
      "hint": "What solution aggregates logs from distributed systems into a central searchable location?",
      "difficulty": "beginner",
      "points": 12
    },
    {
      "id": "alerting-strategy",
      "title": "Setting Up Effective Alerts",
      "description": "What's the best strategy for alerting on application issues?",
      "situation": "Your team gets too many alerts (50+ per day), causing alert fatigue. Critical issues are being missed.",
      "codeExample": "# Current alerts:\n# - Disk usage > 50% (fires constantly)\n# - Any HTTP 404 error\n# - CPU > 30%\n# - Memory usage increases\n# Result: Alert fatigue, missed critical issues",
      "options": [
        "Alert on symptoms (user-facing issues like high error rates, slow response times) not causes, use severity levels, and set meaningful thresholds",
        "Send all alerts to everyone on the team",
        "Disable all alerts to reduce noise",
        "Only alert during business hours"
      ],
      "correctAnswer": 0,
      "explanation": "Effective alerting focuses on symptoms users experience (errors, latency) rather than low-level metrics (CPU, memory). Use severity levels (critical, warning, info), set thresholds based on SLOs, and alert on rate of change rather than absolute values. Implement on-call rotations with escalation policies. This reduces noise while catching real issues.",
      "hint": "What approach reduces alert fatigue while ensuring critical issues are caught?",
      "difficulty": "beginner",
      "points": 12
    },
    {
      "id": "distributed-tracing",
      "title": "Debugging Microservices",
      "description": "A user request touches 8 microservices. One service is slow. How do you identify the bottleneck?",
      "situation": "Request latency is 5 seconds, but you don't know which of the 8 services in the call chain is slow.",
      "codeExample": "# Request flow:\n# API Gateway → Auth → UserService → \n# ProductService → InventoryService → \n# PricingService → CartService → PaymentService\n# Total: 5 seconds (expected: 500ms)",
      "options": [
        "Add timing logs to every service and correlate them manually",
        "Restart all services and hope the issue resolves",
        "Implement distributed tracing with OpenTelemetry, Jaeger, or Zipkin to track requests across services with trace IDs",
        "Check each service individually until you find the slow one"
      ],
      "correctAnswer": 2,
      "explanation": "Distributed tracing instruments each service to propagate trace IDs through the request chain. Tools like Jaeger or Zipkin visualize the entire request flow, showing time spent in each service, dependencies, and bottlenecks. This is essential for debugging microservices architectures where requests span multiple services.",
      "hint": "What technology tracks a single request across multiple services with correlated IDs?",
      "difficulty": "intermediate",
      "points": 14
    },
    {
      "id": "slo-sli-sla",
      "title": "Understanding Service Levels",
      "description": "What's the difference between SLI, SLO, and SLA?",
      "situation": "You need to define reliability targets for your API service.",
      "codeExample": "# Your API:\n# - Current uptime: 99.5%\n# - Customer contract requires 99.9%\n# - You want to track availability",
      "options": [
        "SLI is what you measure (availability %), SLO is your internal target (99.95%), SLA is contractual commitment to customers (99.9%)",
        "They're all the same thing with different names",
        "SLA is more important, ignore SLI and SLO",
        "Only enterprise companies need these, startups don't"
      ],
      "correctAnswer": 0,
      "explanation": "SLI (Service Level Indicator) is the actual measurement (e.g., % of successful requests). SLO (Service Level Objective) is your internal target (99.95% success rate). SLA (Service Level Agreement) is the contractual commitment to customers (99.9%) with penalties if missed. SLOs should be stricter than SLAs to provide a buffer. These form the foundation of site reliability engineering.",
      "hint": "Think about measurements, internal goals, and external commitments. What's the hierarchy?",
      "difficulty": "intermediate",
      "points": 13
    },
    {
      "id": "golden-signals",
      "title": "The Four Golden Signals",
      "description": "Which metrics are most important for monitoring a web service?",
      "situation": "You can only monitor 4 metric categories for your web application. Which should you choose?",
      "options": [
        "CPU, Memory, Disk, Network",
        "Number of users, Revenue, Conversions, Bounce rate",
        "Server count, Container count, Pod count, Node count",
        "Latency, Traffic, Errors, Saturation (The Four Golden Signals)"
      ],
      "correctAnswer": 3,
      "explanation": "The Four Golden Signals from Google's SRE book are: Latency (response time), Traffic (requests/second), Errors (error rate), and Saturation (resource utilization). These metrics indicate user experience and system health better than infrastructure metrics alone. They apply to almost any service and help identify issues quickly.",
      "hint": "What monitoring framework does Google SRE recommend for any service?",
      "difficulty": "intermediate",
      "points": 12
    },
    {
      "id": "alert-fatigue",
      "title": "Preventing Alert Fatigue",
      "description": "Your on-call engineer receives 100 alerts per week. Most aren't actionable. How do you fix this?",
      "situation": "Team morale is low due to constant alerts. Many alerts auto-resolve before anyone can investigate.",
      "codeExample": "# Alert problems:\n# - 60% auto-resolve within 5 minutes\n# - 30% require no action\n# - 10% are actionable issues\n# On-call engineers burning out",
      "options": [
        "Increase alert thresholds so they fire less often",
        "Ignore alerts that auto-resolve",
        "Implement alert aggregation, set minimum duration before alerting, use warning vs critical severity, and create runbooks for each alert",
        "Hire more engineers to handle the alerts"
      ],
      "correctAnswer": 2,
      "explanation": "Combat alert fatigue by: aggregating related alerts, requiring issues persist for minimum duration (e.g., 5 minutes) before alerting, using severity levels appropriately, creating clear runbooks for each alert, and regularly reviewing/removing noisy alerts. Every alert should be actionable and require human intervention. Auto-resolving alerts indicate improper thresholds.",
      "hint": "What combination of techniques makes each alert meaningful and actionable?",
      "difficulty": "intermediate",
      "points": 15
    },
    {
      "id": "observability-pillars",
      "title": "The Three Pillars of Observability",
      "description": "How do metrics, logs, and traces work together in observability?",
      "situation": "You're implementing a complete observability stack. How should you use each pillar?",
      "options": [
        "Only use metrics, logs and traces are redundant",
        "Use metrics to identify WHEN issues occur, logs to understand WHAT happened, traces to see WHERE in distributed systems. Correlate all three for complete observability",
        "Use logs for everything, they contain all information",
        "Metrics, logs, and traces are competing solutions, choose one"
      ],
      "correctAnswer": 1,
      "explanation": "The three pillars complement each other: Metrics show trends and trigger alerts (WHEN), Logs provide detailed context and error messages (WHAT), Traces show request flows through distributed systems (WHERE). Modern observability correlates all three using common identifiers (trace IDs, user IDs) to enable drilling down from high-level metrics to specific log entries and traces.",
      "hint": "How do these three different data types complement each other rather than compete?",
      "difficulty": "advanced",
      "points": 18
    },
    {
      "id": "cardinality-explosion",
      "title": "Managing High-Cardinality Metrics",
      "description": "You want to track request latency by user ID (10 million users). What's the problem?",
      "situation": "Your monitoring system is slow and expensive. You're tracking metrics with high-cardinality labels like user_id, session_id, and request_id.",
      "codeExample": "# Metric:\n# http_request_duration{user_id=\"...\", endpoint=\"...\", method=\"...\"}\n# - 10M users\n# - 100 endpoints  \n# - 4 HTTP methods\n# = 4 billion time series!",
      "options": [
        "This is fine, track everything with maximum detail",
        "Use sampling and exemplars for high-cardinality data, keep metrics aggregated, use logs or traces for individual request details",
        "Delete old metrics to save space",
        "Switch to a more expensive monitoring solution"
      ],
      "correctAnswer": 1,
      "explanation": "High-cardinality labels create massive numbers of time series, crushing performance and costs. Instead: keep metrics aggregated by low-cardinality labels (endpoint, status code), use sampling for high-cardinality data, use exemplars (links from metrics to traces), and store detailed per-request data in logs or traces. This provides necessary detail without overwhelming the metrics system.",
      "hint": "What's the cost-effective way to handle detailed per-request data without creating billions of time series?",
      "difficulty": "advanced",
      "points": 20
    }
  ]
}
